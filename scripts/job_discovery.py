#!/usr/bin/env python3
"""
LinkedIn Job Discovery & Scraping Automation

This script automates job discovery by searching LinkedIn Jobs and scraping
full job descriptions. It deduplicates against existing applications and
saves results to staging/ for analysis.

Usage:
    python scripts/job_discovery.py                    # Interactive mode
    python scripts/job_discovery.py --auto             # Use career-preferences.md
    python scripts/job_discovery.py --keywords "VP Product" --location "London"
"""

import argparse
import json
import re
import time
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional
from playwright.sync_api import sync_playwright, Page, Browser


class LinkedInJobSearcher:
    """Searches LinkedIn Jobs and scrapes job descriptions."""

    BASE_URL = "https://www.linkedin.com/jobs/search/"

    # Date filter codes
    DATE_FILTERS = {
        'past_24h': 'r86400',
        'past_week': 'r604800',
        'past_month': 'r2592000',
        'any_time': ''
    }

    def __init__(self, headless: bool = True, slow_mo: int = 100):
        """
        Initialize the searcher.

        Args:
            headless: Run browser in headless mode
            slow_mo: Slow down operations by N milliseconds (helps avoid detection)
        """
        self.headless = headless
        self.slow_mo = slow_mo
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None

    def build_search_url(
        self,
        keywords: str,
        location: str,
        date_posted: str = 'past_week',
        experience_level: Optional[List[str]] = None
    ) -> str:
        """
        Build LinkedIn job search URL with filters.

        Args:
            keywords: Job title or keywords (e.g., "Director Product Data Platform")
            location: Location (e.g., "London, United Kingdom")
            date_posted: One of: past_24h, past_week, past_month, any_time
            experience_level: List of: entry, associate, mid_senior, director, executive

        Returns:
            Complete search URL
        """
        params = {
            'keywords': keywords.replace(' ', '%20'),
            'location': location.replace(' ', '%20'),
        }

        # Add date filter
        if date_posted in self.DATE_FILTERS:
            date_code = self.DATE_FILTERS[date_posted]
            if date_code:
                params['f_TPR'] = date_code

        # Add experience level filter
        if experience_level:
            # LinkedIn codes: 2=Entry, 3=Associate, 4=Mid-Senior, 5=Director, 6=Executive
            level_codes = {
                'entry': '2',
                'associate': '3',
                'mid_senior': '4',
                'director': '5',
                'executive': '6'
            }
            codes = [level_codes[level] for level in experience_level if level in level_codes]
            if codes:
                params['f_E'] = '%2C'.join(codes)

        # Build URL
        query_string = '&'.join([f"{k}={v}" for k, v in params.items()])
        return f"{self.BASE_URL}?{query_string}"

    def start_browser(self):
        """Start Playwright browser."""
        playwright = sync_playwright().start()
        self.browser = playwright.chromium.launch(
            headless=self.headless,
            slow_mo=self.slow_mo
        )
        self.page = self.browser.new_page()

        # Set user agent to look more human
        self.page.set_extra_http_headers({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })

    def close_browser(self):
        """Close browser."""
        if self.browser:
            self.browser.close()

    def scroll_to_load_all_jobs(self, max_scrolls: int = 10):
        """
        Scroll down to load lazy-loaded job cards.

        Args:
            max_scrolls: Maximum number of scroll actions
        """
        print("  â¬ Scrolling to load all job cards...")
        for i in range(max_scrolls):
            # Scroll down
            self.page.evaluate("window.scrollBy(0, 1000)")
            time.sleep(0.5)

            # Check if we've reached the bottom
            try:
                # LinkedIn shows "No more jobs" or pagination
                if self.page.query_selector('.jobs-search-results__list-item--load-more'):
                    break
            except:
                pass

        print(f"  âœ“ Scrolled {max_scrolls} times to load content")

    def extract_job_cards(self) -> List[Dict[str, str]]:
        """
        Extract job cards from search results page.

        Returns:
            List of job dictionaries with: title, company, location, url
        """
        jobs = []

        # Try multiple selectors (LinkedIn changes them frequently)
        selectors = [
            '.jobs-search__results-list li',
            '.scaffold-layout__list-container li',
            'ul.jobs-search__results-list > li'
        ]

        job_cards = None
        for selector in selectors:
            try:
                job_cards = self.page.query_selector_all(selector)
                if job_cards and len(job_cards) > 0:
                    print(f"  âœ“ Found {len(job_cards)} job cards using selector: {selector}")
                    break
            except:
                continue

        if not job_cards:
            print("  âš ï¸ No job cards found")
            return jobs

        for card in job_cards:
            try:
                # Extract job title
                title_elem = card.query_selector('.job-card-list__title, .base-search-card__title')
                title = title_elem.inner_text().strip() if title_elem else None

                # Extract company
                company_elem = card.query_selector('.job-card-container__company-name, .base-search-card__subtitle')
                company = company_elem.inner_text().strip() if company_elem else None

                # Extract location
                location_elem = card.query_selector('.job-card-container__metadata-item, .job-search-card__location')
                location = location_elem.inner_text().strip() if location_elem else None

                # Extract URL
                link_elem = card.query_selector('a[href*="/jobs/view/"]')
                url = link_elem.get_attribute('href') if link_elem else None

                if title and company and url:
                    # Clean URL (remove tracking parameters)
                    clean_url = url.split('?')[0]

                    jobs.append({
                        'title': title,
                        'company': company,
                        'location': location or 'Not specified',
                        'url': clean_url
                    })

            except Exception as e:
                # Skip cards that fail to parse
                continue

        return jobs

    def search(
        self,
        keywords: str,
        location: str,
        date_posted: str = 'past_week',
        experience_level: Optional[List[str]] = None
    ) -> List[Dict[str, str]]:
        """
        Perform a job search on LinkedIn.

        Args:
            keywords: Job title or keywords
            location: Location string
            date_posted: Time filter
            experience_level: Experience level filters

        Returns:
            List of job dictionaries
        """
        url = self.build_search_url(keywords, location, date_posted, experience_level)

        print(f"\nðŸ” Searching LinkedIn Jobs:")
        print(f"  Keywords: {keywords}")
        print(f"  Location: {location}")
        print(f"  Date: {date_posted}")

        if not self.browser:
            self.start_browser()

        try:
            # Navigate to search URL
            print(f"  ðŸŒ Loading: {url}")
            self.page.goto(url, timeout=30000, wait_until='networkidle')

            # Wait for job cards to load
            time.sleep(2)

            # Scroll to load more results
            self.scroll_to_load_all_jobs()

            # Extract job cards
            jobs = self.extract_job_cards()

            print(f"  âœ… Found {len(jobs)} jobs")
            return jobs

        except Exception as e:
            print(f"  âŒ Search failed: {e}")
            return []

    def scrape_job_description(self, url: str) -> Optional[str]:
        """
        Scrape full job description from a LinkedIn job posting.

        Args:
            url: LinkedIn job URL

        Returns:
            Job description text or None if failed
        """
        if not self.browser:
            self.start_browser()

        try:
            print(f"    ðŸ“„ Scraping: {url}")
            self.page.goto(url, timeout=30000, wait_until='networkidle')
            time.sleep(1)

            # Try multiple selectors for job description
            selectors = [
                '.jobs-description__content',
                '.show-more-less-html__markup',
                '.description__text'
            ]

            description = None
            for selector in selectors:
                try:
                    elem = self.page.query_selector(selector)
                    if elem:
                        description = elem.inner_text()
                        break
                except:
                    continue

            if description:
                print(f"    âœ“ Scraped {len(description)} characters")
                return description
            else:
                print(f"    âš ï¸ No description found")
                return None

        except Exception as e:
            print(f"    âŒ Failed to scrape: {e}")
            return None


class JobDeduplicator:
    """Checks for duplicate jobs against existing applications."""

    def __init__(self, applications_dir: Path):
        """
        Initialize deduplicator.

        Args:
            applications_dir: Path to applications/ folder
        """
        self.applications_dir = applications_dir
        self.existing_jobs = self._load_existing_jobs()

    def _load_existing_jobs(self) -> List[Dict[str, str]]:
        """Load all existing applications."""
        existing = []

        if not self.applications_dir.exists():
            return existing

        for app_folder in self.applications_dir.iterdir():
            if app_folder.is_dir():
                # Extract company and role from folder name
                # Format: YYYY-MM-CompanyName-RoleTitle
                parts = app_folder.name.split('-', 3)
                if len(parts) >= 4:
                    company = parts[2]
                    role = parts[3]
                    existing.append({
                        'company': company.lower(),
                        'role': role.lower(),
                        'folder': app_folder.name
                    })

        return existing

    def is_duplicate(self, company: str, title: str) -> Optional[str]:
        """
        Check if job already exists in applications.

        Args:
            company: Company name
            title: Job title

        Returns:
            Existing folder name if duplicate, None otherwise
        """
        company_clean = company.lower().replace(' ', '')
        title_clean = title.lower().replace(' ', '')

        for existing in self.existing_jobs:
            existing_company = existing['company'].replace(' ', '')
            existing_role = existing['role'].replace(' ', '')

            # Check for close match (allows for slight variations)
            if (existing_company in company_clean or company_clean in existing_company):
                if (existing_role in title_clean or title_clean in existing_role):
                    return existing['folder']

        return None


def sanitize_filename(text: str) -> str:
    """Convert text to safe filename."""
    # Remove invalid characters
    text = re.sub(r'[<>:"/\\|?*]', '', text)
    # Replace spaces and special chars with hyphens
    text = re.sub(r'[\s\-]+', '-', text)
    # Remove leading/trailing hyphens
    text = text.strip('-')
    # Limit length
    return text[:100]


def save_to_staging(
    job: Dict[str, str],
    description: str,
    staging_dir: Path,
    batch_name: str
):
    """
    Save job description to staging folder.

    Args:
        job: Job metadata dictionary
        description: Job description text
        staging_dir: Path to staging directory
        batch_name: Name of this discovery batch
    """
    # Create batch folder
    batch_folder = staging_dir / batch_name
    batch_folder.mkdir(parents=True, exist_ok=True)

    # Create job folder
    company_safe = sanitize_filename(job['company'])
    title_safe = sanitize_filename(job['title'])
    job_folder = batch_folder / f"{company_safe}-{title_safe}"
    job_folder.mkdir(parents=True, exist_ok=True)

    # Save job description
    desc_file = job_folder / "job-description.md"
    with open(desc_file, 'w', encoding='utf-8') as f:
        f.write(f"# {job['title']} @ {job['company']}\n\n")
        f.write(f"**Location:** {job['location']}\n")
        f.write(f"**Source:** {job['url']}\n\n")
        f.write("---\n\n")
        f.write(description)

    print(f"    âœ… Saved to: {job_folder.name}/")


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(description='LinkedIn Job Discovery Automation')
    parser.add_argument('--keywords', type=str, help='Job search keywords')
    parser.add_argument('--location', type=str, help='Job location')
    parser.add_argument('--date', type=str, default='past_week',
                       choices=['past_24h', 'past_week', 'past_month', 'any_time'],
                       help='Date posted filter')
    parser.add_argument('--auto', action='store_true',
                       help='Use career-preferences.md for search criteria')
    parser.add_argument('--headless', action='store_true',
                       help='Run browser in headless mode')

    args = parser.parse_args()

    # Setup paths
    project_root = Path(__file__).parent.parent
    staging_dir = project_root / 'staging'
    applications_dir = project_root / 'applications'

    # Create batch name
    batch_name = datetime.now().strftime('%Y-%m-%d-discovery-batch')

    print("=" * 80)
    print("ðŸ¤– LinkedIn Job Discovery Automation")
    print("=" * 80)

    # Define searches (TODO: Load from career-preferences.md in --auto mode)
    searches = []

    if args.auto:
        # TODO: Parse career-preferences.md for keywords and locations
        print("\nâš ï¸ --auto mode not yet implemented, using defaults")
        searches = [
            {
                'keywords': 'Director Product Data Platform',
                'location': 'London, United Kingdom',
                'experience_level': ['director']
            },
            {
                'keywords': 'Head of Product Growth',
                'location': 'London, United Kingdom',
                'experience_level': ['director']
            }
        ]
    elif args.keywords and args.location:
        searches = [{
            'keywords': args.keywords,
            'location': args.location,
            'experience_level': None
        }]
    else:
        # Interactive mode - use defaults
        searches = [
            {
                'keywords': 'Director Product Data Platform',
                'location': 'London, United Kingdom',
                'experience_level': ['director']
            }
        ]

    # Initialize searcher and deduplicator
    searcher = LinkedInJobSearcher(headless=args.headless, slow_mo=100)
    deduplicator = JobDeduplicator(applications_dir)

    try:
        all_jobs = []

        # Perform searches
        for search in searches:
            jobs = searcher.search(
                keywords=search['keywords'],
                location=search['location'],
                date_posted=args.date,
                experience_level=search.get('experience_level')
            )
            all_jobs.extend(jobs)

        # Deduplicate
        print(f"\nðŸ“Š Total jobs discovered: {len(all_jobs)}")

        new_jobs = []
        duplicates = []

        for job in all_jobs:
            existing = deduplicator.is_duplicate(job['company'], job['title'])
            if existing:
                duplicates.append({**job, 'existing_folder': existing})
            else:
                new_jobs.append(job)

        print(f"âœ… New jobs: {len(new_jobs)}")
        print(f"â­ï¸  Duplicates (already tracked): {len(duplicates)}")

        if duplicates:
            print("\n  Skipped duplicates:")
            for dup in duplicates[:5]:  # Show first 5
                print(f"    - {dup['company']} - {dup['title']} (exists: {dup['existing_folder']})")
            if len(duplicates) > 5:
                print(f"    ... and {len(duplicates) - 5} more")

        # Scrape full descriptions for new jobs
        if new_jobs:
            print(f"\nðŸ“¥ Scraping full job descriptions for {len(new_jobs)} new jobs...")

            scraped_count = 0
            for i, job in enumerate(new_jobs, 1):
                print(f"\n  [{i}/{len(new_jobs)}] {job['company']} - {job['title']}")

                description = searcher.scrape_job_description(job['url'])

                if description:
                    save_to_staging(job, description, staging_dir, batch_name)
                    scraped_count += 1

                    # Rate limiting - pause between requests
                    if i < len(new_jobs):
                        time.sleep(2)

            print(f"\nâœ… Successfully scraped {scraped_count}/{len(new_jobs)} jobs")
            print(f"ðŸ“ Saved to: {staging_dir / batch_name}")

            # Save summary
            summary_file = staging_dir / batch_name / "DISCOVERY-SUMMARY.json"
            summary = {
                'batch_name': batch_name,
                'date': datetime.now().isoformat(),
                'searches': searches,
                'total_discovered': len(all_jobs),
                'new_jobs': len(new_jobs),
                'duplicates': len(duplicates),
                'scraped': scraped_count,
                'jobs': new_jobs
            }

            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary, f, indent=2)

            print(f"ðŸ“„ Summary saved: {summary_file.name}")

            # Next steps
            print("\n" + "=" * 80)
            print("ðŸŽ¯ Next Steps:")
            print("=" * 80)
            print(f"1. Run bulk analysis: python scripts/bulk_analyze.py {staging_dir / batch_name}")
            print(f"2. Review results in: {staging_dir / batch_name}/BULK-ANALYSIS-SUMMARY.md")
            print(f"3. Apply to 8+ fit score roles using /generate-cv and /generate-cover-letter")

        else:
            print("\nâœ¨ No new jobs found (all are duplicates)")

    finally:
        searcher.close_browser()
        print("\nâœ… Done!")


if __name__ == '__main__':
    main()
